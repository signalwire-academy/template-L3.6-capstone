name: Grade Submission

on:
  push:
    branches: [main]
    paths:
      - 'solution/**'
  workflow_dispatch:

permissions:
  contents: read
  issues: write
  checks: write

env:
  PYTHON_VERSION: '3.11'

jobs:
  grade:
    name: Auto-Grade
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -q signalwire-agents pyyaml

      - name: Check solution exists
        id: check
        run: |
          if [ -f "solution/agent.py" ]; then
            if grep -qE "(AgentBase|from signalwire)" solution/agent.py; then
              echo "exists=true" >> $GITHUB_OUTPUT
            else
              echo "exists=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Run grading
        if: steps.check.outputs.exists == 'true'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python << 'GRADESCRIPT'
          import subprocess
          import json
          import yaml
          import sys
          import os
          import urllib.request
          import urllib.error

          with open('tests/grading.yaml') as f:
              config = yaml.safe_load(f)

          results = {
              'assignment': config['assignment'],
              'checks': [],
              'score': 0,
              'max_score': 0,
              'passed': False
          }

          agent_file = 'solution/agent.py'

          swml_result = subprocess.run(
              ['swaig-test', agent_file, '--dump-swml', '--raw'],
              capture_output=True, text=True, timeout=30
          )
          swml_data = None
          swml_text = ""
          if swml_result.returncode == 0:
              try:
                  swml_data = json.loads(swml_result.stdout)
                  swml_text = swml_result.stdout.lower()
              except:
                  pass

          tools_result = subprocess.run(
              ['swaig-test', agent_file, '--list-tools'],
              capture_output=True, text=True, timeout=30
          )
          tools_text = tools_result.stdout.lower() if tools_result.returncode == 0 else ""

          for check in config['checks']:
              check_result = {
                  'id': check['id'],
                  'name': check['name'],
                  'max_points': check['points'],
                  'points': 0,
                  'passed': False,
                  'output': ''
              }

              try:
                  if check['type'] == 'instantiate':
                      check_result['passed'] = tools_result.returncode == 0
                      if not check_result['passed']:
                          check_result['output'] = tools_result.stderr[:200]

                  elif check['type'] == 'swml_valid':
                      check_result['passed'] = swml_data is not None and 'sections' in swml_data

                  elif check['type'] == 'swml_contains':
                      if swml_text:
                          check_result['passed'] = True
                          for req in check.get('require', []):
                              text = req.get('text', '').lower()
                              if text and text not in swml_text:
                                  check_result['passed'] = False
                                  check_result['output'] = f"'{text}' not found"
                                  break

                  elif check['type'] == 'function_exists':
                      func = check.get('function', '')
                      check_result['passed'] = func.lower() in tools_text
                      if not check_result['passed']:
                          check_result['output'] = f"Function '{func}' not found"

                  elif check['type'] == 'exec':
                      cmd = ['swaig-test', agent_file, '--exec', check['function']]
                      for key, value in check.get('args', {}).items():
                          cmd.extend([f'--{key}', str(value)])
                      result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                      if result.returncode == 0:
                          output_lower = result.stdout.lower()
                          expected = check.get('expect', {}).get('stdout_contains', [])
                          check_result['passed'] = all(exp.lower() in output_lower for exp in expected)
                          if not check_result['passed']:
                              check_result['output'] = f"Missing: {expected}"
                      else:
                          check_result['output'] = result.stderr[:200]

              except Exception as e:
                  check_result['output'] = str(e)[:200]

              if check_result['passed']:
                  check_result['points'] = check['points']
              results['checks'].append(check_result)

          results['max_score'] = sum(c['max_points'] for c in results['checks'])
          results['score'] = sum(c['points'] for c in results['checks'])
          results['percentage'] = round(results['score'] / results['max_score'] * 100, 1) if results['max_score'] > 0 else 0
          results['passed'] = results['percentage'] >= config['assignment']['passing_score']

          with open('results.json', 'w') as f:
              json.dump(results, f, indent=2)

          print(f"Score: {results['score']}/{results['max_score']} ({results['percentage']}%)")

          report = f"## Grading Results\n\n"
          report += f"**Assignment:** {results['assignment']['name']}\n"
          report += f"**Score:** {results['score']}/{results['max_score']} ({results['percentage']}%)\n"
          report += f"**Status:** {'PASSED' if results['passed'] else 'NOT PASSING'}\n\n"
          report += "### Checks\n\n| # | Check | Points | Status |\n|---|-------|--------|--------|\n"
          for i, c in enumerate(results['checks']):
              report += f"| {i+1} | {c['name']} | {c['points']}/{c['max_points']} | {'[x]' if c['passed'] else '[ ]'} |\n"
          failed = [c for c in results['checks'] if not c['passed'] and c['output']]
          if failed:
              report += "\n### Details\n\n"
              for c in failed:
                  report += f"**{c['name']}:** {c['output']}\n\n"
          report += "\n---\n" + (config.get('feedback', {}).get('pass', '') if results['passed'] else config.get('feedback', {}).get('fail', ''))
          from datetime import datetime
          report += f"\n\n---\n*Graded: {datetime.utcnow().isoformat()}Z*"

          token = os.environ.get('GITHUB_TOKEN')
          repo = os.environ.get('GITHUB_REPOSITORY', '')
          if token and repo:
              headers = {"Authorization": f"Bearer {token}", "Accept": "application/vnd.github+json", "X-GitHub-Api-Version": "2022-11-28"}
              try:
                  req = urllib.request.Request(f"https://api.github.com/repos/{repo}/issues?labels=grading&state=open", headers=headers)
                  with urllib.request.urlopen(req) as resp:
                      issues = json.loads(resp.read().decode())
                  if issues:
                      url = f"https://api.github.com/repos/{repo}/issues/{issues[0]['number']}/comments"
                  else:
                      url = f"https://api.github.com/repos/{repo}/issues"
                  data = json.dumps({"body": report} if issues else {"title": "Grading Results", "body": report, "labels": ["grading"]}).encode()
                  req = urllib.request.Request(url, data=data, headers={**headers, "Content-Type": "application/json"})
                  urllib.request.urlopen(req)
              except Exception as e:
                  print(f"Could not post to GitHub: {e}")
          GRADESCRIPT

      - name: Upload results
        if: steps.check.outputs.exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: grading-results
          path: results.json
          retention-days: 90
